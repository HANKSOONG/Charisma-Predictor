{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JTvauGrH4Ww"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqmX2g8kH9DL"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner\n",
        "!pip install joblib\n",
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDJ1jk_GW-kq"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Compressed file paths\n",
        "zip_file_paths = [\n",
        "    '/content/drive/MyDrive/Data/Facial_key_point_data_second.zip',\n",
        "    '/content/drive/MyDrive/Data/val_data.zip',\n",
        "    '/content/drive/MyDrive/Data/test_data.zip'\n",
        "]\n",
        "\n",
        "# Corresponding unzipped folder paths\n",
        "extracted_folder_paths = [\n",
        "    '/content/training_data',\n",
        "    '/content/val_data',\n",
        "    '/content/testing_data'\n",
        "]\n",
        "\n",
        "def unzip_file(zip_file, extract_to):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to)\n",
        "\n",
        "# Unzip the files\n",
        "for zip_file, extracted_folder in zip(zip_file_paths, extracted_folder_paths):\n",
        "    unzip_file(zip_file, extracted_folder)\n",
        "    print(f\"Decompression completed for {zip_file}!\")\n",
        "\n",
        "print(\"All decompressions completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWzIy8gTtXey"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "\n",
        "import json\n",
        "import csv\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization,\n",
        "    LSTM, GRU, Bidirectional, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, RepeatVector\n",
        ")\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "\n",
        "# Suppress warnings and debug messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAdhMWkbD_Gi"
      },
      "outputs": [],
      "source": [
        "# Function to retrieve all file names in the specified folder\n",
        "def get_file_names(folder_path):\n",
        "    file_names = []\n",
        "    for entry in os.listdir(folder_path):\n",
        "        full_path = os.path.join(folder_path, entry)\n",
        "        if os.path.isfile(full_path):\n",
        "            file_names.append(entry)\n",
        "    return file_names\n",
        "\n",
        "# Function to retrieve file names without their extension\n",
        "def get_file_names_without_extension(folder_path, extension=\".json\"):\n",
        "    file_names = []\n",
        "    for entry in os.listdir(folder_path):\n",
        "        if entry.endswith(extension):\n",
        "            file_names.append(os.path.splitext(entry)[0])\n",
        "    return file_names\n",
        "\n",
        "# Load annotation data from CSV\n",
        "def load_annotation_data(csv_path, file_names):\n",
        "    annotation_dict = {}\n",
        "    with open(csv_path, 'r') as csvfile:\n",
        "        datareader = csv.reader(csvfile)\n",
        "        data = list(datareader)\n",
        "\n",
        "    for name in file_names:\n",
        "        for row in data:\n",
        "            if row[0] == name + '.mp4':\n",
        "                annotation_dict[row[0]] = {\n",
        "                    'extraversion': float(row[1]),\n",
        "                    'neuroticism': float(row[2]),\n",
        "                    'agreeableness': float(row[3]),\n",
        "                    'conscientiousness': float(row[4]),\n",
        "                    'openness': float(row[5])\n",
        "                }\n",
        "    return annotation_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LK_k5yFEWUh"
      },
      "outputs": [],
      "source": [
        "# Load facial landmarks data from JSON files\n",
        "def load_facial_landmarks_data(folder_path, file_list):\n",
        "    data_list = []\n",
        "    valid_file_list = []  # List to store files that have corresponding data\n",
        "    for file_ in file_list:\n",
        "        try:\n",
        "            with open(os.path.join(folder_path, file_), 'r') as file:\n",
        "                data = json.load(file)\n",
        "\n",
        "            all_frames = []\n",
        "            for frame in data:\n",
        "                landmarks = sorted(frame['landmarks'], key=lambda k: k['index'])  # Ensure landmarks are sorted by index\n",
        "                coordinates = [coord for landmark in landmarks for coord in (landmark['x'], landmark['y'], landmark['z'])]\n",
        "                all_frames.append(coordinates)\n",
        "\n",
        "            matrix = np.array(all_frames)\n",
        "            if matrix.size > 0:\n",
        "                data_list.append(matrix)\n",
        "                valid_file_list.append(file_)\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading {file_}: {e}\")\n",
        "    return data_list, valid_file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DYtzY0wEej-"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the dataset\n",
        "def preprocess_dataset(folder_path, csv_path, scaler=None):\n",
        "    # Get the list of file names\n",
        "    file_list = get_file_names(folder_path)\n",
        "    print(\"Files found in the folder:\", file_list)\n",
        "\n",
        "    # Get the list of file names without extension\n",
        "    file_names = get_file_names_without_extension(folder_path)\n",
        "\n",
        "    # Load annotation data\n",
        "    annotation_dict = load_annotation_data(csv_path, file_names)\n",
        "    print(\"Annotation dictionary:\", annotation_dict)\n",
        "\n",
        "    # Load facial landmarks data\n",
        "    data_list, valid_file_list = load_facial_landmarks_data(folder_path, file_list)\n",
        "    print(\"Shape of the matrix:\", len(data_list))\n",
        "\n",
        "    label_list = []  # List to store labels\n",
        "\n",
        "    for file_ in valid_file_list:\n",
        "        try:\n",
        "            # Get the file name without extension to match with dictionary keys\n",
        "            file_name = os.path.splitext(file_)[0]\n",
        "            # Fetch the labels for the corresponding file\n",
        "            if file_name + '.mp4' in annotation_dict:\n",
        "                labels = annotation_dict[file_name + '.mp4']\n",
        "                label_list.append(labels)  # Append labels to the label list\n",
        "            else:\n",
        "                print(f\"Warning: No labels found for {file_name}.mp4\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing file {file_}: {e}\")\n",
        "\n",
        "    # Ensure consistent lengths of data_list and label_list\n",
        "    assert len(data_list) == len(label_list), \"Inconsistent lengths of data and labels\"\n",
        "\n",
        "    # Now data_list contains the matrices and label_list contains the corresponding labels\n",
        "    print(\"First label:\", label_list[0])\n",
        "\n",
        "    # Convert labels list to a pandas DataFrame for easier manipulation\n",
        "    labels_df = pd.DataFrame(label_list)\n",
        "\n",
        "    # Plot the distribution of each personality trait\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
        "    traits = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
        "    for i, trait in enumerate(traits):\n",
        "        row = i // 2\n",
        "        col = i % 2\n",
        "        sns.histplot(labels_df[trait], bins=30, kde=True, ax=axes[row, col])\n",
        "        axes[row, col].set_title(f'Distribution of {trait}')\n",
        "        axes[row, col].set_xlabel('Score')\n",
        "        axes[row, col].set_ylabel('Frequency')\n",
        "\n",
        "    # Remove the last subplot if there are only five traits\n",
        "    fig.delaxes(axes[2][1])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find the maximum length of any matrix in the list\n",
        "    max_length = max(len(matrix) for matrix in data_list)\n",
        "\n",
        "    # Assuming that each matrix has the same number of columns, find the number of columns of the first non-empty matrix\n",
        "    for matrix in data_list:\n",
        "        if matrix.shape[0] > 0:\n",
        "            num_features = matrix.shape[1]\n",
        "            break\n",
        "    else:\n",
        "        # If all matrices are empty, set a default number of columns\n",
        "        num_features = 0\n",
        "\n",
        "    # Normalize each matrix\n",
        "    if scaler is None:\n",
        "        scaler = StandardScaler()\n",
        "        normalized_data_list = []\n",
        "        for matrix in data_list:\n",
        "            if matrix.shape[0] > 0:\n",
        "                # Flatten the matrix, fit_transform, then reshape back\n",
        "                original_shape = matrix.shape\n",
        "                matrix = scaler.fit_transform(matrix.reshape(-1, matrix.shape[-1])).reshape(original_shape)\n",
        "            normalized_data_list.append(matrix)\n",
        "    else:\n",
        "        normalized_data_list = []\n",
        "        for matrix in data_list:\n",
        "            if matrix.shape[0] > 0:\n",
        "                original_shape = matrix.shape\n",
        "                matrix = scaler.transform(matrix.reshape(-1, matrix.shape[-1])).reshape(original_shape)\n",
        "            normalized_data_list.append(matrix)\n",
        "\n",
        "    # Fill each matrix to the same number of frames (max_length)\n",
        "    padded_data_list = []\n",
        "    for matrix in normalized_data_list:\n",
        "        if matrix.shape[0] == 0:\n",
        "            # If the matrix is empty, create a zero matrix of shape (max_length, num_features)\n",
        "            padded_matrix = np.zeros((max_length, num_features))\n",
        "        else:\n",
        "            padding_length = max_length - matrix.shape[0]\n",
        "            padded_matrix = np.pad(matrix, ((0, padding_length), (0, 0)), 'constant', constant_values=0)\n",
        "        padded_data_list.append(padded_matrix)\n",
        "\n",
        "    # Convert lists to NumPy arrays\n",
        "    padded_data_array = np.array(padded_data_list)\n",
        "    labels_array = np.array([list(map(float, [labels['extraversion'], labels['neuroticism'], labels['agreeableness'], labels['conscientiousness'], labels['openness']])) for labels in label_list])\n",
        "\n",
        "    return padded_data_array, labels_array, valid_file_list, scaler, max_length, num_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVocz6FoFYn3"
      },
      "outputs": [],
      "source": [
        "# Preprocess training set\n",
        "train_folder_path = '/content/training_data'\n",
        "train_csv_path = '/content/drive/MyDrive/Data/annotation_training .csv'\n",
        "X_train, y_train, train_file_list, scaler, max_length, num_features = preprocess_dataset(train_folder_path, train_csv_path)\n",
        "\n",
        "# Preprocess validation set\n",
        "val_folder_path = '/content/val_data'\n",
        "val_csv_path = '/content/drive/MyDrive/Data/annotation_validation.csv'\n",
        "X_val, y_val, val_file_list, _, _, _ = preprocess_dataset(val_folder_path, val_csv_path, scaler)\n",
        "\n",
        "# Preprocess testing set\n",
        "test_folder_path = '/content/testing_data'\n",
        "test_csv_path = '/content/drive/MyDrive/Data/annotation_testing.csv'\n",
        "X_test, y_test, test_file_list, _, _, _ = preprocess_dataset(test_folder_path, test_csv_path, scaler)\n",
        "\n",
        "# Convert data to TensorFlow tensors\n",
        "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
        "X_val = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
        "X_test = tf.convert_to_tensor(X_test, dtype=tf.float32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "y_val = tf.convert_to_tensor(y_val, dtype=tf.float32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.float32)\n",
        "\n",
        "print(\"Preprocessing complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, LSTM, GRU, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import itertools\n",
        "import os\n",
        "import csv\n",
        "\n",
        "def plot_loss(history, model_name):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Train Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title(f'{model_name} Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def lr_schedule(epoch):\n",
        "    initial_lr = 0.001\n",
        "    if epoch < 10:\n",
        "        return initial_lr * (epoch + 1) / 10\n",
        "    return initial_lr * 0.1 ** (epoch // 30)\n",
        "\n",
        "def build_model(model_type, max_length, num_features, config):\n",
        "    inputs = Input(shape=(max_length, num_features))\n",
        "\n",
        "    if model_type == 'CNN':\n",
        "        x = inputs\n",
        "        for filters, kernel_size in config:\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = MaxPooling1D(pool_size=2)(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "        x = Flatten()(x)\n",
        "    elif model_type == 'LSTM':\n",
        "        x = inputs\n",
        "        for units in config:\n",
        "            x = LSTM(units=units, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "        x = Flatten()(x)\n",
        "    elif model_type == 'TCN':\n",
        "        x = inputs\n",
        "        for filters, kernel_size in config:\n",
        "            x = Conv1D(filters=filters, kernel_size=kernel_size, activation='relu', padding='causal', kernel_regularizer=l2(0.01))(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "        x = Flatten()(x)\n",
        "    elif model_type == 'Transformer':\n",
        "        x = inputs\n",
        "        for units in config:\n",
        "            x = Dense(units, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "            attention_output = MultiHeadAttention(num_heads=1, key_dim=units)(x, x)\n",
        "            x = BatchNormalization()(attention_output)\n",
        "            x = Dropout(0.5)(x)\n",
        "        x = GlobalAveragePooling1D()(x)\n",
        "    elif model_type == 'GRU':\n",
        "        x = inputs\n",
        "        for units in config:\n",
        "            x = GRU(units=units, return_sequences=True, kernel_regularizer=l2(0.01))(x)\n",
        "            x = Dropout(0.5)(x)\n",
        "        x = Flatten()(x)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    x = Dense(units=32, activation='relu', kernel_regularizer=l2(0.01))(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(units=5, activation='sigmoid')(x)  # Adjust output units to match the number of traits\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='mean_absolute_error', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "def train_model(model, X_train, y_train, X_val, y_val, model_name):\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)  # Increased patience\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)  # Increased patience\n",
        "    lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "    history = model.fit(X_train, y_train, epochs=200, batch_size=64, validation_data=(X_val, y_val),  # Increased epochs\n",
        "                        callbacks=[early_stopping, reduce_lr, lr_scheduler])\n",
        "    plot_loss(history, model_name)\n",
        "    return model\n",
        "\n",
        "# Load annotation data from CSV\n",
        "def load_annotation_data(csv_path):\n",
        "    annotation_dict = {}\n",
        "    with open(csv_path, 'r') as csvfile:\n",
        "        datareader = csv.reader(csvfile)\n",
        "        next(datareader)  # Skip the header row\n",
        "        for row in datareader:\n",
        "            annotation_dict[row[0]] = {\n",
        "                'extraversion': float(row[1]),\n",
        "                'neuroticism': float(row[2]),\n",
        "                'agreeableness': float(row[3]),\n",
        "                'conscientiousness': float(row[4]),\n",
        "                'openness': float(row[5])\n",
        "            }\n",
        "    return annotation_dict\n",
        "\n",
        "# Function to retrieve file names without their extension\n",
        "def get_file_names_without_extension(folder_path, extension=\".json\"):\n",
        "    file_names = []\n",
        "    for entry in os.listdir(folder_path):\n",
        "        if entry.endswith(extension):\n",
        "            file_names.append(os.path.splitext(entry)[0])\n",
        "    return file_names\n",
        "\n",
        "# Assuming X_train, X_val, X_test, y_train, y_val, y_test are already defined\n",
        "# Define variables max_length and num_features based on your data\n",
        "max_length = X_train.shape[1]\n",
        "num_features = X_train.shape[2]\n",
        "\n",
        "# Define four different configurations for each model type\n",
        "configurations = {\n",
        "    'CNN': [[(16, 3)], [(32, 3)], [(16, 3), (32, 3)], [(32, 3), (64, 3)]],\n",
        "    'LSTM': [[16], [32], [16, 32], [32, 64]],\n",
        "    'TCN': [[(16, 3)], [(32, 3)], [(16, 3), (32, 3)], [(32, 3), (64, 3)]],\n",
        "    'Transformer': [[16], [32], [16, 32], [32, 64]],\n",
        "    'GRU': [[16], [32], [16, 32], [32, 64]]\n",
        "}\n",
        "\n",
        "best_models = {}\n",
        "best_model_mae = {}\n",
        "\n",
        "# Train models with the defined configurations and select the best one for each type\n",
        "for model_type in configurations:\n",
        "    best_mae = float('inf')\n",
        "    best_model = None\n",
        "    print(f\"\\nTraining {model_type} models with different configurations...\")\n",
        "\n",
        "    for i, config in enumerate(configurations[model_type]):\n",
        "        print(f\"\\nTraining {model_type} model with configuration {i+1}...\")\n",
        "        model = build_model(model_type, max_length, num_features, config)\n",
        "        trained_model = train_model(model, X_train, y_train, X_val, y_val, f\"{model_type}_config_{i+1}\")\n",
        "        trained_models[model_type] = trained_model\n",
        "\n",
        "        # Evaluate the model on the validation set\n",
        "        val_predictions = trained_model.predict(X_val)\n",
        "        val_mae = mean_absolute_error(y_val, val_predictions)\n",
        "\n",
        "        if val_mae < best_mae:\n",
        "            best_mae = val_mae\n",
        "            best_model = trained_model\n",
        "\n",
        "    best_models[model_type] = best_model\n",
        "    best_model_mae[model_type] = best_mae\n",
        "\n",
        "# Generate predictions on the test set using the best models\n",
        "test_predictions = {model: best_models[model].predict(X_test) for model in best_models}\n",
        "\n",
        "# Evaluate each model's MAE for each trait\n",
        "model_mae = {model: [] for model in best_models}\n",
        "for model in best_models:\n",
        "    for i in range(y_test.shape[1]):\n",
        "        mae = mean_absolute_error(y_test[:, i], test_predictions[model][:, i])\n",
        "        model_mae[model].append(mae)\n",
        "\n",
        "# Print each model's MAE\n",
        "for model in best_models:\n",
        "    print(f\"\\n{model} MAE for each trait:\")\n",
        "    for i, trait in enumerate(['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']):\n",
        "        print(f\"  {trait}: {model_mae[model][i]:.4f}\")\n",
        "\n",
        "# Simple averaging\n",
        "simple_avg_predictions = np.mean([test_predictions[model] for model in best_models], axis=0)\n",
        "simple_avg_mae = mean_absolute_error(y_test, simple_avg_predictions)\n",
        "print(f\"\\nSimple average MAE: {simple_avg_mae:.4f}\")\n",
        "\n",
        "# Evaluate different weight combinations\n",
        "def evaluate_combination(weights):\n",
        "    combined_prediction = np.zeros_like(test_predictions['CNN'])\n",
        "    for i, model in enumerate(best_models):\n",
        "        combined_prediction += weights[i] * test_predictions[model]\n",
        "    mae = mean_absolute_error(y_test, combined_prediction)\n",
        "    return mae\n",
        "\n",
        "# Generate all possible weight combinations\n",
        "weight_combinations = list(itertools.product(np.arange(0, 1.1, 0.1), repeat=len(best_models)))\n",
        "weight_combinations = [weights for weights in weight_combinations if np.isclose(sum(weights), 1.0)]\n",
        "\n",
        "# Try different weight combinations and find the top 5 combinations with the lowest MAE\n",
        "best_combinations = []\n",
        "for weights in weight_combinations:\n",
        "    mae = evaluate_combination(weights)\n",
        "    best_combinations.append((weights, mae))\n",
        "\n",
        "best_combinations = sorted(best_combinations, key=lambda x: x[1])[:5]\n",
        "\n",
        "print(\"\\nTop 5 weight combinations:\")\n",
        "for weights, mae in best_combinations:\n",
        "    print(f\"Weights: {weights}, MAE: {mae:.4f}\")\n",
        "\n",
        "# Use the best weight combination to generate the final combined prediction\n",
        "best_weights = best_combinations[0][0]\n",
        "final_prediction = np.zeros_like(test_predictions['CNN'])\n",
        "for i, model in enumerate(best_models):\n",
        "    final_prediction += best_weights[i] * test_predictions[model]\n",
        "\n",
        "# Calculate MAE for each trait\n",
        "final_mae_per_trait = [mean_absolute_error(y_test[:, i], final_prediction[:, i]) for i in range(y_test.shape[1])]\n",
        "print(\"\\nFinal MAE per trait:\")\n",
        "for i, trait in enumerate(['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']):\n",
        "    print(f\"  {trait}: {final_mae_per_trait[i]:.4f}\")"
      ],
      "metadata": {
        "id": "hHTZK0OQnOcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate MAE for each trait\n",
        "final_mae_per_trait = [mean_absolute_error(y_test[:, i], final_prediction[:, i]) for i in range(y_test.shape[1])]\n",
        "print(\"\\nFinal MAE per trait:\")\n",
        "for i, trait in enumerate(['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']):\n",
        "    print(f\"  {trait}: {final_mae_per_trait[i]:.4f}\")\n",
        "\n",
        "# Retrieve file names for the final predictions CSV\n",
        "annotation_data = load_annotation_data('/content/drive/MyDrive/Data/annotation_testing.csv')\n",
        "file_names = list(annotation_data.keys())\n",
        "\n",
        "# Ensure the length of file_names matches the number of predictions\n",
        "if len(file_names) != final_prediction.shape[0]:\n",
        "    missing_count = final_prediction.shape[0] - len(file_names)\n",
        "    if missing_count > 0:\n",
        "        missing_files = list(annotation_data.keys())[:missing_count]  # Get missing file names\n",
        "        file_names.extend(missing_files)\n",
        "    elif missing_count < 0:\n",
        "        file_names = file_names[:final_prediction.shape[0]]\n",
        "\n",
        "# Save the final prediction to CSV\n",
        "final_prediction_df = pd.DataFrame(final_prediction, columns=['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness'])\n",
        "final_prediction_df['file_name'] = file_names  # Add the file names to the DataFrame\n",
        "final_prediction_df.to_csv('/content/drive/MyDrive/Data/final_predictions.csv', index=False)\n",
        "\n",
        "print(\"Final predictions saved to 'final_predictions.csv'\")"
      ],
      "metadata": {
        "id": "F45odbusoJbT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}