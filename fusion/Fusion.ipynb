{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EfhXxzGVI70Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A_4qso968WHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive (if not already mounted)\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define file paths for predictions\n",
        "pred_video_file_path = '/content/drive/MyDrive/project/pred_video.csv'\n",
        "pred_audio_file_path = '/content/drive/MyDrive/project/pred_audio.csv'\n",
        "pred_text_file_path = '/content/drive/MyDrive/project/pred_text.csv'\n",
        "\n",
        "\n",
        "# Load predictions as pandas DataFrames\n",
        "pred_video_df = pd.read_csv(pred_video_file_path)\n",
        "pred_audio_df = pd.read_csv(pred_audio_file_path)\n",
        "pred_text_df = pd.read_csv(pred_text_file_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "42dOvhE2pqLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the true labels into a pandas DataFrame\n",
        "true_labels_df = pd.read_csv('/content/drive/MyDrive/Data/annotation_testing.csv')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(true_labels_df.head())\n"
      ],
      "metadata": {
        "id": "5henX_QaFRG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove file extensions from the 'Filename' column\n",
        "pred_video_df['Filename'] = pred_video_df['Filename'].str.replace('.json', '', regex=False)\n",
        "pred_audio_df['Filename'] = pred_audio_df['Filename'].str.replace('.wav', '', regex=False)\n",
        "pred_text_df['Filename'] = pred_text_df['Filename'].str.replace('.mp4', '', regex=False)\n",
        "true_labels_df['Filename'] = true_labels_df['Filename'].str.replace('.mp4', '', regex=False)\n",
        "\n",
        "# Display the first few rows of each DataFrame after removing extensions\n",
        "print(\"Video Predictions:\")\n",
        "print(pred_video_df.head())\n",
        "\n",
        "print(\"\\nAudio Predictions:\")\n",
        "print(pred_audio_df.head())\n",
        "\n",
        "print(\"\\nText Predictions:\")\n",
        "print(pred_text_df.head())\n",
        "\n",
        "print(\"\\nAnnotation Testing Labels:\")\n",
        "print(true_labels_df.head())\n"
      ],
      "metadata": {
        "id": "SYoq0flcj8gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge"
      ],
      "metadata": {
        "id": "fmGIL7nYFTU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Video Predictions:\")\n",
        "print(pred_video_df.head())\n",
        "\n",
        "print(\"\\nAudio Predictions:\")\n",
        "print(pred_audio_df.head())\n",
        "\n",
        "print(\"\\nText Predictions:\")\n",
        "print(pred_text_df.head())\n",
        "\n",
        "print(\"\\nAnnotation Testing Labels:\")\n",
        "print(true_labels_df.head())"
      ],
      "metadata": {
        "id": "TBtvtPMxkFhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_predictions_and_labels(video_df, audio_df, text_df, labels_df):\n",
        "\n",
        "    # Merge video, audio, and text predictions on 'filename'\n",
        "    combined_df = pd.merge(video_df, audio_df, on='Filename', suffixes=('_video', '_audio'))\n",
        "    combined_df = pd.merge(combined_df, text_df, on='Filename')\n",
        "    combined_df.rename(columns={'leadership_score': 'leadership_score_text'}, inplace=True)\n",
        "\n",
        "    # Merge the combined predictions with the true labels\n",
        "    final_df = pd.merge(combined_df, labels_df, on='Filename', suffixes=('_text', '_true'))\n",
        "\n",
        "    return final_df\n",
        "\n",
        "# Merge DataFrames\n",
        "final_df = merge_predictions_and_labels(pred_video_df, pred_audio_df, pred_text_df, true_labels_df)\n",
        "\n",
        "# Display the final DataFrame\n",
        "print(final_df)"
      ],
      "metadata": {
        "id": "mesYHRD6pyKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_video_df['Filename'].head())\n",
        "print(pred_audio_df['Filename'].head())\n",
        "print(pred_text_df['Filename'].head())\n",
        "print(true_labels_df['Filename'].head())\n"
      ],
      "metadata": {
        "id": "m8KoubsBF-AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = final_df.drop(['layer_5', 'layer_7'], axis=1)"
      ],
      "metadata": {
        "id": "Lj6l8faHTjki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df.columns)\n",
        "columns_to_drop = ['layer_5', 'layer_7']\n",
        "final_df = final_df.drop(columns=[col for col in columns_to_drop if col in final_df.columns])\n"
      ],
      "metadata": {
        "id": "7aozxvj4G4QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Average**"
      ],
      "metadata": {
        "id": "224QnuE_FcGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "## Extract the predicted scores\n",
        "predicted_video = final_df['leadership_score_video'].values\n",
        "predicted_audio = final_df['leadership_score_audio'].values\n",
        "predicted_text = final_df['leadership_score_text'].values\n",
        "\n",
        "# Calculate average predicted scores\n",
        "average_predicted_scores = (predicted_video + predicted_audio + predicted_text) / 3\n",
        "\n",
        "# Extract the true labels\n",
        "y_true = final_df['leadership_score'].values\n",
        "\n",
        "# Calculate regression metrics based on average predicted scores\n",
        "mae = metrics.mean_absolute_error(y_true, average_predicted_scores)\n",
        "mse = metrics.mean_squared_error(y_true, average_predicted_scores)\n",
        "r2 = metrics.r2_score(y_true, average_predicted_scores)\n",
        "\n",
        "# Print regression metrics\n",
        "print('Mean Absolute Error (MAE):', mae)\n",
        "print('Mean Squared Error (MSE):', mse)\n",
        "print('R^2 Score:', r2)"
      ],
      "metadata": {
        "id": "YlvaDTAIV-ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "\n",
        "# Assuming final_df contains the data\n",
        "predicted_extraversion = (final_df['extraversion_video'].values + final_df['extraversion_audio'].values + final_df['extraversion_text'].values) / 3\n",
        "\n",
        "predicted_neuroticism = (final_df['neuroticism_video'].values + final_df['neuroticism_audio'].values + final_df['neuroticism_text'].values) / 3\n",
        "\n",
        "predicted_agreeableness = (final_df['agreeableness_video'].values + \\\n",
        "                           final_df['agreeableness_audio'].values + \\\n",
        "                           final_df['agreeableness_text'].values) / 3\n",
        "\n",
        "predicted_conscientiousness = (final_df['conscientiousness_video'].values + \\\n",
        "                              final_df['conscientiousness_audio'].values + \\\n",
        "                              final_df['conscientiousness_text'].values) / 3\n",
        "\n",
        "predicted_openness = (final_df['openness_video'].values + \\\n",
        "                     final_df['openness_audio'].values + \\\n",
        "                     final_df['openness_text'].values) / 3\n",
        "\n",
        "# Extract the true labels\n",
        "extraversion_true = final_df['extraversion_true'].values\n",
        "neuroticism_true = final_df['neuroticism_true'].values\n",
        "agreeableness_true = final_df['agreeableness_true'].values\n",
        "conscientiousness_true = final_df['conscientiousness_true'].values\n",
        "openness_true = final_df['openness_true'].values\n",
        "\n",
        "# Calculate regression metrics for each personality score\n",
        "metrics_extraversion = {\n",
        "    'MAE': metrics.mean_absolute_error(extraversion_true, predicted_extraversion),\n",
        "    'MSE': metrics.mean_squared_error(extraversion_true, predicted_extraversion),\n",
        "    'R^2': metrics.r2_score(extraversion_true, predicted_extraversion)\n",
        "}\n",
        "\n",
        "metrics_neuroticism = {\n",
        "    'MAE': metrics.mean_absolute_error(neuroticism_true, predicted_neuroticism),\n",
        "    'MSE': metrics.mean_squared_error(neuroticism_true, predicted_neuroticism),\n",
        "    'R^2': metrics.r2_score(neuroticism_true, predicted_neuroticism)\n",
        "}\n",
        "\n",
        "metrics_agreeableness = {\n",
        "    'MAE': metrics.mean_absolute_error(agreeableness_true, predicted_agreeableness),\n",
        "    'MSE': metrics.mean_squared_error(agreeableness_true, predicted_agreeableness),\n",
        "    'R^2': metrics.r2_score(agreeableness_true, predicted_agreeableness)\n",
        "}\n",
        "\n",
        "metrics_conscientiousness = {\n",
        "    'MAE': metrics.mean_absolute_error(conscientiousness_true, predicted_conscientiousness),\n",
        "    'MSE': metrics.mean_squared_error(conscientiousness_true, predicted_conscientiousness),\n",
        "    'R^2': metrics.r2_score(conscientiousness_true, predicted_conscientiousness)\n",
        "}\n",
        "\n",
        "metrics_openness = {\n",
        "    'MAE': metrics.mean_absolute_error(openness_true, predicted_openness),\n",
        "    'MSE': metrics.mean_squared_error(openness_true, predicted_openness),\n",
        "    'R^2': metrics.r2_score(openness_true, predicted_openness)\n",
        "}\n",
        "\n",
        "# Print regression metrics for each personality score\n",
        "print('Metrics for Extraversion:')\n",
        "for metric_name, value in metrics_extraversion.items():\n",
        "    print(f'{metric_name}: {value:.4f}')\n",
        "\n",
        "print('\\nMetrics for Neuroticism:')\n",
        "for metric_name, value in metrics_neuroticism.items():\n",
        "    print(f'{metric_name}: {value:.4f}')\n",
        "\n",
        "print('\\nMetrics for Agreeableness:')\n",
        "for metric_name, value in metrics_agreeableness.items():\n",
        "    print(f'{metric_name}: {value:.4f}')\n",
        "\n",
        "print('\\nMetrics for Conscientiousness:')\n",
        "for metric_name, value in metrics_conscientiousness.items():\n",
        "    print(f'{metric_name}: {value:.4f}')\n",
        "\n",
        "print('\\nMetrics for Openness:')\n",
        "for metric_name, value in metrics_openness.items():\n",
        "    print(f'{metric_name}: {value:.4f}')\n"
      ],
      "metadata": {
        "id": "PP-oZsoCFdux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weighted"
      ],
      "metadata": {
        "id": "oVOlKorXA_Ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import product\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "# Extract the predicted scores\n",
        "predicted_video = final_df['leadership_score_video'].values\n",
        "predicted_audio = final_df['leadership_score_audio'].values\n",
        "predicted_text = final_df['leadership_score_text'].values\n",
        "\n",
        "# Extract the true labels\n",
        "y_true = final_df['leadership_score'].values\n",
        "\n",
        "# Generate all possible weight combinations\n",
        "def generate_triplets():\n",
        "    numbers = [0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]\n",
        "    triplets = []\n",
        "\n",
        "    for triplet in product(numbers, repeat=3):\n",
        "        if sum(triplet) == 1:\n",
        "            triplets.append(triplet)\n",
        "\n",
        "    return triplets\n",
        "\n",
        "# Function to calculate metrics for each weight set\n",
        "def calculate_metrics(weight_sets, predicted_video, predicted_audio, predicted_text, y_true):\n",
        "    mae_scores = []\n",
        "    mse_scores = []\n",
        "    r2_scores = []\n",
        "\n",
        "    for weights in weight_sets:\n",
        "        w_video, w_audio, w_text = weights\n",
        "        average_predicted_scores = (w_video * predicted_video + w_audio * predicted_audio + w_text * predicted_text)\n",
        "\n",
        "        mae = metrics.mean_absolute_error(y_true, average_predicted_scores)\n",
        "        mse = metrics.mean_squared_error(y_true, average_predicted_scores)\n",
        "        r2 = metrics.r2_score(y_true, average_predicted_scores)\n",
        "\n",
        "        mae_scores.append(mae)\n",
        "        mse_scores.append(mse)\n",
        "        r2_scores.append(r2)\n",
        "\n",
        "    return mae_scores, mse_scores, r2_scores\n",
        "\n",
        "# Generate weight sets\n",
        "weight_sets = generate_triplets()\n",
        "\n",
        "# Calculate metrics\n",
        "mae_scores, mse_scores, r2_scores = calculate_metrics(weight_sets, predicted_video, predicted_audio, predicted_text, y_true)\n",
        "\n",
        "# Sort indices for MAE and MSE (smallest first)\n",
        "sorted_indices_mae = np.argsort(mae_scores)[:10]\n",
        "sorted_indices_mse = np.argsort(mse_scores)[:10]\n",
        "\n",
        "# Sort indices for R2 (largest first)\n",
        "sorted_indices_r2 = np.argsort(r2_scores)[-10:]\n",
        "\n",
        "# Extract sorted scores and labels\n",
        "sorted_mae_scores = [mae_scores[i] for i in sorted_indices_mae]\n",
        "sorted_mse_scores = [mse_scores[i] for i in sorted_indices_mse]\n",
        "sorted_r2_scores = [r2_scores[i] for i in sorted_indices_r2]\n",
        "\n",
        "sorted_weight_labels_mae = [f'Weights: {weight_sets[i]}' for i in sorted_indices_mae]\n",
        "sorted_weight_labels_mse = [f'Weights: {weight_sets[i]}' for i in sorted_indices_mse]\n",
        "sorted_weight_labels_r2 = [f'Weights: {weight_sets[i]}' for i in sorted_indices_r2]\n",
        "\n",
        "# Plotting the sorted metrics\n",
        "fig, axs = plt.subplots(3, 1, figsize=(10, 20))\n",
        "\n",
        "# MAE plot\n",
        "axs[0].barh(sorted_weight_labels_mae[::-1], sorted_mae_scores[::-1], color='blue', alpha=0.7)\n",
        "axs[0].set_title('Top 10 Smallest Mean Absolute Error (MAE)')\n",
        "axs[0].set_xlabel('MAE')\n",
        "axs[0].set_ylabel('Weight Sets')\n",
        "\n",
        "# MSE plot\n",
        "axs[1].barh(sorted_weight_labels_mse[::-1], sorted_mse_scores[::-1], color='green', alpha=0.7)\n",
        "axs[1].set_title('Top 10 Smallest Mean Squared Error (MSE)')\n",
        "axs[1].set_xlabel('MSE')\n",
        "axs[1].set_ylabel('Weight Sets')\n",
        "\n",
        "# R2 plot\n",
        "axs[2].barh(sorted_weight_labels_r2, sorted_r2_scores, color='orange', alpha=0.7)\n",
        "axs[2].set_title('Top 10 Largest R-squared (R2)')\n",
        "axs[2].set_xlabel('R2')\n",
        "axs[2].set_ylabel('Weight Sets')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TuFbdNpxhrcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the weight sets based on MAE (ascending), MSE (ascending), and R2 (descending)\n",
        "sorted_indices_mae = np.argsort(mae_scores)\n",
        "sorted_indices_mse = np.argsort(mse_scores)\n",
        "sorted_indices_r2 = np.argsort(r2_scores)[::-1]  # Descending order for R2\n",
        "\n",
        "# Extract the top 5 weight sets based on each metric\n",
        "top_n = 5\n",
        "\n",
        "top_indices_mae = sorted_indices_mae[:top_n]\n",
        "top_indices_mse = sorted_indices_mse[:top_n]\n",
        "top_indices_r2 = sorted_indices_r2[:top_n]\n",
        "\n",
        "# Print the top 5 weight sets and their corresponding metrics\n",
        "print(\"Top 5 Weight Sets based on MAE:\")\n",
        "for idx in top_indices_mae:\n",
        "    print(f\"Weights: {weight_sets[idx]} - MAE: {mae_scores[idx]:.4f}, MSE: {mse_scores[idx]:.4f}, R2: {r2_scores[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 5 Weight Sets based on MSE:\")\n",
        "for idx in top_indices_mse:\n",
        "    print(f\"Weights: {weight_sets[idx]} - MAE: {mae_scores[idx]:.4f}, MSE: {mse_scores[idx]:.4f}, R2: {r2_scores[idx]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 5 Weight Sets based on R2:\")\n",
        "for idx in top_indices_r2:\n",
        "    print(f\"Weights: {weight_sets[idx]} - MAE: {mae_scores[idx]:.4f}, MSE: {mse_scores[idx]:.4f}, R2: {r2_scores[idx]:.4f}\")"
      ],
      "metadata": {
        "id": "Mmz4wjvxhxO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "leadership level"
      ],
      "metadata": {
        "id": "MzPrV9grOrR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(weight_sets[top_indices_mae[0]])"
      ],
      "metadata": {
        "id": "k_NfC50LOLmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the predicted weighted results\n",
        "weighted_predictions_df = pd.read_csv('/content/weighted_predictions.csv')\n",
        "\n",
        "# Reading real tags\n",
        "true_labels_df = pd.read_csv('/content/drive/MyDrive/Data/annotation_testing.csv')\n",
        "\n",
        "# Make sure the column names are consistent and all lowercase\n",
        "weighted_predictions_df.columns = weighted_predictions_df.columns.str.lower()\n",
        "true_labels_df.columns = true_labels_df.columns.str.lower()\n",
        "\n",
        "# Display data to confirm\n",
        "print(weighted_predictions_df.head())\n",
        "print(true_labels_df.head())\n",
        "from sklearn import metrics\n",
        "\n",
        "# Defining the Big Five Personality Traits\n",
        "traits = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
        "\n",
        "# Calculating MAE and MSE\n",
        "def calculate_mae_mse(pred_df, true_df, traits):\n",
        "    results = {}\n",
        "    for trait in traits:\n",
        "        y_true = true_df[trait].values\n",
        "        y_pred = pred_df[f'{trait}_weighted'].values\n",
        "        mae = metrics.mean_absolute_error(y_true, y_pred)\n",
        "        mse = metrics.mean_squared_error(y_true, y_pred)\n",
        "        results[trait] = {'MAE': mae, 'MSE': mse}\n",
        "    return results\n",
        "\n",
        "# Calculate MAE and MSE for weighted forecasts\n",
        "weighted_metrics = calculate_mae_mse(weighted_predictions_df, true_labels_df, traits)\n",
        "\n",
        "# Print the MAE and MSE of the weighted forecast\n",
        "for trait, metrics in weighted_metrics.items():\n",
        "    print(f\"\\nWeighted Metrics for {trait.capitalize()}:\")\n",
        "    for metric_name, value in metrics.items():\n",
        "        print(f\"{metric_name}: {value:.4f}\")\n"
      ],
      "metadata": {
        "id": "OxuGvAldvS4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_leadership_score(df):\n",
        "    # Convert column names to lowercase\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df.rename(columns={'extroversion': 'extraversion'}, inplace=True)\n",
        "\n",
        "    # Calculate leadership score\n",
        "    df['leadership_score'] = (df['extraversion']*0.3261) + \\\n",
        "                             (df['neuroticism']*(-0.087)) + \\\n",
        "                             (df['agreeableness']*0.1957) + \\\n",
        "                             (df['conscientiousness']*0.2283) + \\\n",
        "                             (df['openness']*0.163)\n",
        "    return df\n",
        "\n",
        "def leadership_level(score):\n",
        "    if 0.8 <= score <= 1.0:\n",
        "        return 'Level 1 (0.8-1)'\n",
        "    elif 0.6 <= score < 0.8:\n",
        "        return 'Level 2 (0.6-0.8)'\n",
        "    elif 0.4 <= score < 0.6:\n",
        "        return 'Level 3 (0.4-0.6)'\n",
        "    elif 0.2 <= score < 0.4:\n",
        "        return 'Level 4 (0.2-0.4)'\n",
        "    elif 0 <= score < 0.2:\n",
        "        return 'Level 5 (0-0.2)'\n",
        "    else:\n",
        "        return 'Unknown'\n"
      ],
      "metadata": {
        "id": "kTfR3fiQjUSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the combination with the smallest MAE\n",
        "best_weights = weight_sets[top_indices_mae[0]]\n",
        "\n",
        "# Define the list of traits and their respective columns\n",
        "traits = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
        "trait_columns = ['{}_video', '{}_audio', '{}_text']\n",
        "\n",
        "# Calculate predicted scores for each Big Five personality trait using the best weights\n",
        "for trait in traits:\n",
        "    final_df[f'{trait}_pred'] = sum(best_weights[i] * final_df[trait_col.format(trait)] for i, trait_col in enumerate(trait_columns))\n",
        "\n",
        "# Define weights for the Big Five personality traits\n",
        "weights = {\n",
        "    'extraversion': 0.3,\n",
        "    'neuroticism': 0.1,\n",
        "    'agreeableness': 0.2,\n",
        "    'conscientiousness': 0.25,\n",
        "    'openness': 0.15\n",
        "}\n",
        "\n",
        "# Calculate leadership score\n",
        "final_df['leadership_score'] = sum(weights[trait] * final_df[f'{trait}_pred'] for trait in traits)\n",
        "\n",
        "# Define a function for leadership levels\n",
        "def leadership_level(score):\n",
        "    if 0.8 <= score <= 1.0:\n",
        "        return 'Level 1 (0.8-1)'\n",
        "    elif 0.6 <= score < 0.8:\n",
        "        return 'Level 2 (0.6-0.8)'\n",
        "    elif 0.4 <= score < 0.6:\n",
        "        return 'Level 3 (0.4-0.6)'\n",
        "    elif 0.2 <= score < 0.4:\n",
        "        return 'Level 4 (0.2-0.4)'\n",
        "    elif 0 <= score < 0.2:\n",
        "        return 'Level 5 (0-0.2)'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Apply the function to compute leadership levels\n",
        "final_df['leadership_level'] = final_df['leadership_score'].apply(leadership_level)\n",
        "\n",
        "# Print the results\n",
        "print(final_df[['filename', 'leadership_score', 'leadership_level']])\n",
        "\n",
        "# Generate histogram for leadership scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(final_df['leadership_score'], bins=20, color='blue', alpha=0.7)\n",
        "plt.title('Distribution of Leadership Scores')\n",
        "plt.xlabel('Leadership Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Generate bar plot for leadership levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "final_df['leadership_level'].value_counts().sort_index().plot(kind='bar', color='green', alpha=0.7)\n",
        "plt.title('Distribution of Leadership Levels')\n",
        "plt.xlabel('Leadership Level')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8SixlQPUJX_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Big Five personality traits and their corresponding columns\n",
        "traits = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
        "\n",
        "# Function to calculate leadership score\n",
        "def calculate_leadership_score(df):\n",
        "    weights = {\n",
        "        'extraversion': 0.3,\n",
        "        'neuroticism': 0.1,\n",
        "        'agreeableness': 0.2,\n",
        "        'conscientiousness': 0.25,\n",
        "        'openness': 0.15\n",
        "    }\n",
        "    df['leadership_score'] = sum(weights[trait] * df[f'{trait}_pred'] for trait in traits)\n",
        "    return df\n",
        "\n",
        "# Function to define leadership levels\n",
        "def leadership_level(score):\n",
        "    if 0.8 <= score <= 1.0:\n",
        "        return 'Level 1 (0.8-1)'\n",
        "    elif 0.6 <= score < 0.8:\n",
        "        return 'Level 2 (0.6-0.8)'\n",
        "    elif 0.4 <= score < 0.6:\n",
        "        return 'Level 3 (0.4-0.6)'\n",
        "    elif 0.2 <= score < 0.4:\n",
        "        return 'Level 4 (0.2-0.4)'\n",
        "    elif 0 <= score < 0.2:\n",
        "        return 'Level 5 (0-0.2)'\n",
        "    else:\n",
        "        return 'Unknown'\n",
        "\n",
        "# Read true labels from CSV\n",
        "true_labels_df = pd.read_csv('/content/drive/MyDrive/Data/annotation_testing.csv')\n",
        "\n",
        "# Use true labels directly as predicted scores for each Big Five personality trait\n",
        "for trait in traits:\n",
        "    true_labels_df[f'{trait}_pred'] = true_labels_df[trait]\n",
        "\n",
        "# Calculate leadership score\n",
        "true_labels_df = calculate_leadership_score(true_labels_df)\n",
        "\n",
        "# Apply function to compute leadership levels\n",
        "true_labels_df['leadership_level'] = true_labels_df['leadership_score'].apply(leadership_level)\n",
        "\n",
        "# Print results\n",
        "print(true_labels_df[['Filename', 'leadership_score', 'leadership_level']])\n",
        "\n",
        "# Generate histogram of true leadership scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(true_labels_df['leadership_score'], bins=20, color='red', alpha=0.7)\n",
        "plt.title('Distribution of True Leadership Scores')\n",
        "plt.xlabel('Leadership Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Generate bar plot of true leadership levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "true_labels_df['leadership_level'].value_counts().sort_index().plot(kind='bar', color='purple', alpha=0.7)\n",
        "plt.title('Distribution of True Leadership Levels')\n",
        "plt.xlabel('Leadership Level')\n",
        "plt.ylabel('Count')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Compare distribution of predicted and true leadership scores\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(final_df['leadership_score'], bins=20, color='blue', alpha=0.7, label='Predicted')\n",
        "plt.hist(true_labels_df['leadership_score'], bins=20, color='red', alpha=0.5, label='True')\n",
        "plt.title('Comparison of Leadership Scores Distribution')\n",
        "plt.xlabel('Leadership Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Compare distribution of predicted and true leadership levels\n",
        "plt.figure(figsize=(10, 6))\n",
        "final_df['leadership_level'].value_counts().sort_index().plot(kind='bar', color='blue', alpha=0.7, position=0, width=0.4, label='Predicted')\n",
        "true_labels_df['leadership_level'].value_counts().sort_index().plot(kind='bar', color='red', alpha=0.5, position=1, width=0.4, label='True')\n",
        "plt.title('Comparison of Leadership Levels Distribution')\n",
        "plt.xlabel('Leadership Level')\n",
        "plt.ylabel('Count')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jCSABK3tKVJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming you have already found the best weights based on MAE\n",
        "best_weights = weight_sets[top_indices_mae[0]]  # Using the weights with the smallest MAE\n",
        "\n",
        "# Define Big Five personality traits and their corresponding columns\n",
        "traits = ['extraversion', 'neuroticism', 'agreeableness', 'conscientiousness', 'openness']\n",
        "trait_columns = ['{}_video', '{}_audio', '{}_text']\n",
        "\n",
        "# Calculate weighted predictions for each Big Five trait\n",
        "for trait in traits:\n",
        "    final_df[f'{trait}_weighted'] = sum(best_weights[i] * final_df[trait_col.format(trait)] for i, trait_col in enumerate(trait_columns))\n",
        "\n",
        "# Calculate average predictions for each Big Five trait\n",
        "for trait in traits:\n",
        "    final_df[f'{trait}_average'] = final_df[[trait_col.format(trait) for trait_col in trait_columns]].mean(axis=1)\n",
        "\n",
        "# Create a new DataFrame to save weighted prediction results\n",
        "weighted_prediction_df = final_df[['filename'] + [f'{trait}_weighted' for trait in traits]]\n",
        "\n",
        "# Rename columns for clarity\n",
        "weighted_prediction_df.columns = ['Filename'] + [f'{trait.capitalize()}_Weighted' for trait in traits]\n",
        "\n",
        "# Export to CSV file\n",
        "weighted_prediction_df.to_csv('weighted_predictions.csv', index=False)\n",
        "\n",
        "# Print to confirm results\n",
        "print(weighted_prediction_df.head())\n",
        "\n",
        "# Create a new DataFrame to save average prediction results\n",
        "average_prediction_df = final_df[['filename'] + [f'{trait}_average' for trait in traits]]\n",
        "\n",
        "# Rename columns for clarity\n",
        "average_prediction_df.columns = ['Filename'] + [f'{trait.capitalize()}_Average' for trait in traits]\n",
        "\n",
        "# Export to CSV file\n",
        "average_prediction_df.to_csv('average_predictions.csv', index=False)\n",
        "\n",
        "# Print to confirm results\n",
        "print(average_prediction_df.head())\n"
      ],
      "metadata": {
        "id": "KF4qXr_NvWGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}